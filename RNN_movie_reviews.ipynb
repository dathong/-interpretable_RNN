{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "RNN_movie_reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n7lhjshDRjRq"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLj-gsxQRjM6",
        "colab_type": "text"
      },
      "source": [
        "# Predicting the Sentiment of Movie Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oLFUS8HRjM9",
        "colab_type": "text"
      },
      "source": [
        "There are two goals for this analysis. The first is to accurately predict the sentiment of movie reviews, and the second is to develop my model in such a way that its outputs can be analyzed with TensorBoard. This is the first time that I am using TensorBoard, so I want to have a somewhat challenging task, and not use a huge dataset. There are 25,000 training and testing reviews, so this model can train multiple iterations overnight on my MacBook Pro. The data is provided by a Kaggle competition from 2015 (https://www.kaggle.com/c/word2vec-nlp-tutorial). Despite it having concluded, it can still be used as an excellent learning opportunity. The sections of this analysis are:\n",
        "- Inspect the Data\n",
        "- Clean and Format the Data\n",
        "- Build and Train the Model\n",
        "- Make the Predictions\n",
        "- Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbhIGGIuRjM_",
        "colab_type": "code",
        "outputId": "bede40a8-b99a-41ea-e886-159ea9d0c566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "! pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-eRBGEDRjNO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "df9ffb55-d3e1-49b7-9776-c02c8516d523"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk, re, time\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import namedtuple"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5X2-HAxRjNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data\n",
        "train_data = pd.read_csv(\"./drive/My Drive/deep_learning/research/labeledTrainData.tsv\", delimiter=\"\\t\")\n",
        "test_data = pd.read_csv(\"./drive/My Drive/deep_learning/research/testData.tsv\", delimiter=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubDcFVPRWuVO",
        "colab_type": "code",
        "outputId": "d80c1bc0-4d67-48bb-dc80-7444f24f6914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV8G3jSgRjNc",
        "colab_type": "text"
      },
      "source": [
        "# Inspect the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjKGU8wNRjNf",
        "colab_type": "code",
        "outputId": "fc7b1f4d-de6c-4371-b7be-59e487c96693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3  3630_4          0  It must be assumed that those who praised this...\n",
              "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBPaUOjyRjNn",
        "colab_type": "code",
        "outputId": "7bd5e22d-22d5-45f7-b81e-9694a7be422a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12311_10</td>\n",
              "      <td>Naturally in a film who's main themes are of m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8348_2</td>\n",
              "      <td>This movie is a disaster within a disaster fil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5828_4</td>\n",
              "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7186_2</td>\n",
              "      <td>Afraid of the Dark left me with the impression...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12128_7</td>\n",
              "      <td>A very accurate depiction of small time mob li...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id                                             review\n",
              "0  12311_10  Naturally in a film who's main themes are of m...\n",
              "1    8348_2  This movie is a disaster within a disaster fil...\n",
              "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
              "3    7186_2  Afraid of the Dark left me with the impression...\n",
              "4   12128_7  A very accurate depiction of small time mob li..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3Rb_sgRjN8",
        "colab_type": "code",
        "outputId": "e8a2aef3-7f26-4e3c-bc6f-9d5b6b90878a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 3)\n",
            "(25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5yH1bNYRjOI",
        "colab_type": "text"
      },
      "source": [
        "The reviews are rather long, so we won't be using all of the text to train our model. Using all of the text would increase our training to a longer timeframe than I would rather give to this project, but it should make the predictions more accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHz7ziOwRjOo",
        "colab_type": "text"
      },
      "source": [
        "# Clean and Format the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU-oz87dRjOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text, remove_stopwords=True):\n",
        "    '''Clean the text, with the option to remove stopwords'''\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"<br />\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z.]\", \" \", text)\n",
        "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
        "    text = re.sub(r\"  \", \" \", text)\n",
        "    # Remove punctuation from text\n",
        "    text = ''.join([c for c in text if (c not in punctuation) or (c == '.')])\n",
        "\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DSi0ittMK1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class My_tokenizer:\n",
        "\ttoken_dict = {'unk':1}\n",
        "\tcurrent_dict_size = 0\n",
        "\tdef _init_(self,text):\n",
        "\t\tself.fit_dict_from_text(text)\n",
        "\n",
        "\tdef fit_dict_from_text(self,text,with_dot = False):\n",
        "\t\tif with_dot:\n",
        "\t\t\ttext = text.replace(\".\",\" . \")\n",
        "\t\tfor token in text.split():\n",
        "\t\t\tif token not in self.token_dict:\n",
        "\t\t\t\tself.token_dict[token] = self.current_dict_size + 1\n",
        "\t\t\t\tself.current_dict_size += 1\n",
        "\n",
        "\tdef fit_dict_from_list(self,text_list,with_dot = False):\n",
        "\t\tfor t in text_list:\n",
        "\t\t\tself.fit_dict_from_text(t,with_dot)\n",
        "\n",
        "\tdef get_dict(self):\n",
        "\t\treturn self.token_dict\n",
        "\n",
        "\tdef gen(self,text,with_dot=False):\n",
        "\t\tres = []\n",
        "\t\tif with_dot:\n",
        "\t\t\ttext = text.replace(\".\",\" . \")\n",
        "\t\tfor t in text.split():\n",
        "\t\t\tif t in self.token_dict:\n",
        "\t\t\t\tres.append(self.token_dict[t])\n",
        "\t\t\telse:\n",
        "\t\t\t\tres.append(1)\n",
        "\t\treturn res\n",
        "\n",
        "\tdef gen_list(self,text_list,with_dot=False):\n",
        "\t\ttotal_res = []\n",
        "\t\tfor text in text_list:\n",
        "\t\t\tres = []\n",
        "\t\t\tif with_dot:\n",
        "\t\t\t\ttext = text.replace(\".\",\" . \")\n",
        "\t\t\tfor t in text.split():\n",
        "\t\t\t\tif t in self.token_dict:\n",
        "\t\t\t\t\tres.append(self.token_dict[t])\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tres.append(1)\n",
        "\t\t\ttotal_res.append(res)\n",
        "\t\treturn total_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIxnjYxQRjOv",
        "colab_type": "text"
      },
      "source": [
        "Clean the training and testing reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k4HTGPja9ut",
        "colab_type": "code",
        "outputId": "374bd2b6-27bb-42d7-e875-dd40e333b557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print('test review = ',test_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test review =               id                                             review\n",
            "0      12311_10  Naturally in a film who's main themes are of m...\n",
            "1        8348_2  This movie is a disaster within a disaster fil...\n",
            "2        5828_4  All in all, this is a movie for kids. We saw i...\n",
            "3        7186_2  Afraid of the Dark left me with the impression...\n",
            "4       12128_7  A very accurate depiction of small time mob li...\n",
            "...         ...                                                ...\n",
            "24995   2155_10  Sony Pictures Classics, I'm looking at you! So...\n",
            "24996     59_10  I always felt that Ms. Merkerson had never got...\n",
            "24997    2531_1  I was so disappointed in this movie. I am very...\n",
            "24998    7772_8  From the opening sequence, filled with black a...\n",
            "24999  11465_10  This is a great horror film for people who don...\n",
            "\n",
            "[25000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Jkr6GkbWkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_test = ['I really love this movie. It is awesome',\n",
        "           'I enjoy this movie so much. What a masterpiece. Everything is excellent, the actors are very talented',\n",
        "           'This movie sucks. It is too bad. What a waste of time. It is terrible',\n",
        "           'I will never watch it again. Why the actors are so bad',\n",
        "           'I think this movie is fine']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGTR0FcbYBrJ",
        "colab_type": "code",
        "outputId": "47ca1edb-09b5-4cdd-c359-63768a2424a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iUoc-TWRjOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean = []\n",
        "for review in train_data.review:\n",
        "    train_clean.append(clean_text(review))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqcxKNfuRjO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_clean = []\n",
        "for review in test_data.review:\n",
        "    test_clean.append(clean_text(review))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaUZCSW5boPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_test_clean = []\n",
        "for review in my_test:\n",
        "    my_test_clean.append(clean_text(review))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA404o-uRjO_",
        "colab_type": "code",
        "outputId": "22748885-8234-4757-a44d-3b276c6510e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Tokenize the reviews\n",
        "all_reviews = train_clean + test_clean\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_reviews)\n",
        "print(\"Fitting is complete.\")\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(train_clean)\n",
        "print(\"train_seq is complete.\")\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_clean)\n",
        "print(\"test_seq is complete\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting is complete.\n",
            "train_seq is complete.\n",
            "test_seq is complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm_ItxOFMZTy",
        "colab_type": "code",
        "outputId": "d11825a8-f225-4956-f4f3-307c1d1a56aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "my_tokenizer = My_tokenizer()\n",
        "my_tokenizer.fit_dict_from_list(all_reviews, with_dot=True)\n",
        "print(\"Fitting is complete.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J4IoibCcOlS",
        "colab_type": "code",
        "outputId": "4fd004c2-e17b-41d6-c458-e21c248c5386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "my_test_seq = tokenizer.texts_to_sequences(my_test_clean)\n",
        "print(\"my_test_seq is complete\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_test_seq is complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WvC63xoRXcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c93c1416-b176-4f74-f635-8f3b7e37e10d"
      },
      "source": [
        "dot_index = my_tokenizer.get_dict()[\".\"]\n",
        "print('dot_index = ',dot_index)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot_index =  18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDOB5OqY8jmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff0b27ac-5fd6-4a18-e4c5-874f9c2205c5"
      },
      "source": [
        "n_words = len(my_tokenizer.get_dict())\n",
        "print('n_words = ',n_words)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_words =  99427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_w_v8BxQ-YW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9d074f93-a38c-4e80-b9b8-13968cf28909"
      },
      "source": [
        "my_test_clean"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['really love movie. awesome',\n",
              " 'enjoy movie much. masterpiece. everything excellent actors talented',\n",
              " 'movie sucks. bad. waste time. terrible',\n",
              " 'never watch again. actors bad',\n",
              " 'think movie fine']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nMUxo5LS1Pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "863457ae-31b1-4c54-a8ec-a47173799759"
      },
      "source": [
        "train_clean[:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stuff going moment mj i ve started listening music watching odd documentary there watched wiz watched moonwalker again. maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent. moonwalker part biography part feature film remember going see cinema originally released. subtle messages mj s feeling towards press also obvious message drugs bad m kay. visually impressive course michael jackson unless remotely like mj anyway going hate find boring. may call mj egotist consenting making movie mj fans would say made fans true really nice him. the actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord. wants mj dead bad beyond me. mj overheard plans nah joe pesci s character ranted wanted people know supplying drugs etc dunno maybe hates mj s music. lots cool things like mj turning car robot whole speed demon sequence. also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene. bottom line movie people like mj one level another which think people . not stay away. try give wholesome message ironically mj s bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention i ve gave subject....hmmm well know people different behind closed doors know fact. either extremely nice stupid guy one sickest liars. hope latter.',\n",
              " ' the classic war worlds timothy hines entertaining film obviously goes great effort lengths faithfully recreate h. g. wells classic book. mr. hines succeeds so. i watched film me appreciated fact standard predictable hollywood fare comes every year e.g. spielberg version tom cruise slightest resemblance book. obviously everyone looks different things movie. envision amateur critics look criticize everything can. others rate movie important bases like entertained people never agree critics . enjoyed effort mr. hines put faithful h.g. wells classic novel found entertaining. made easy overlook critics perceive shortcomings. ',\n",
              " 'film starts manager nicholas bell giving welcome investors robert carradine primal park . secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one nature s fearsome predators sabretooth tiger smilodon . scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific.meanwhile youngsters enter restricted area security center attacked pack large pre historical animals deadlier bigger . addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons. sabretooths course real star stars astounding terrifyingly though convincing. giant animals savagely stalking prey group run afoul fight one nature s fearsome predators. furthermore third sabretooth dangerous slow stalks victims. the movie delivers goods lots blood gore beheading hair raising chills full scares sabretooths appear mediocre special effects.the story provides exciting stirring entertainment results quite boring .the giant animals majority made computer generator seem totally lousy .middling performances though players reacting appropriately becoming food.actors give vigorously physical performances dodging beasts running bound leaps dangling walls . packs ridiculous final deadly scene. small kids realistic gory violent attack scenes . films sabretooths smilodon following sabretooth  by james r hickox vanessa angel david keith john rhys davies much better . bc  roland emmerich steven strait cliff curtis camilla belle. motion picture filled bloody moments badly directed george miller originality takes many elements previous films. miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe . rating average bottom barrel.',\n",
              " 'must assumed praised film the greatest filmed opera ever read somewhere either care opera care wagner care anything except desire appear cultured. either representation wagner s swan song movie strikes unmitigated disaster leaden reading score matched tricksy lugubrious realisation text. it s questionable people ideas opera or matter play especially one shakespeare about allowed anywhere near theatre film studio syberberg fashionably without smallest justification wagner s text decided parsifal about bisexual integration title character latter stages transmutes kind beatnik babe though one continues sing high tenor actors film singers get double dose armin jordan conductor seen face but heard voice amfortas also appears monstrously double exposure kind batonzilla conductor ate monsalvat playing good friday music which way transcendant loveliness nature represented scattering shopworn flaccid crocuses stuck ill laid turf expedient baffles me. theatre sometimes piece imperfections thoughts can t think syberberg splice in parsifal gurnemanz mountain pasture lush provided julie andrews sound music... the sound hard endure high voices trumpets particular possessing aural glare adds another sort fatigue impatience uninspired conducting paralytic unfolding ritual. someone another review mentioned bayreuth recording knappertsbusch though tempi often slow jordan altogether lacks sense pulse feeling ebb flow music and half century orchestral sound set modern pressings still superior film. ',\n",
              " 'superbly trashy wondrously unpretentious s exploitation hooray pre credits opening sequences somewhat give false impression we re dealing serious harrowing drama need fear barely ten minutes later we re necks nonsensical chainsaw battles rough fist fights lurid dialogs gratuitous nudity bo ingrid two orphaned siblings unusually close even slightly perverted relationship. imagine playfully ripping towel covers sister s naked body stare unshaven genitals several whole minutes well bo sister and judging dubbed laughter mind all. sick dude anyway kids fled russia parents nasty soldiers brutally slaughtered mommy daddy. friendly smuggler took custody them however even raised trained bo ingrid expert smugglers. actual plot lifts off  years later they re facing ultimate quest mythical incredibly valuable white fire diamond coincidentally found mine. things life ever made little sense plot narrative structure white fire sure lot fun watch. time clue who s beating cause and bet actors understood even less whatever violence magnificently grotesque every single plot twist pleasingly retarded. script goes totally bonkers beyond repair suddenly reveal reason bo needs replacement ingrid fred williamson enters scene big cigar mouth sleazy black fingers local prostitutes. bo s principal opponent italian chick big breasts hideous accent preposterous catchy theme song plays least dozen times throughout film there s obligatory we re falling in love montage loads attractions god brilliant experience. original french title translates life survive uniquely appropriate makes much sense rest movie none ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhmaGMqtRcwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "394a9c36-3cae-44dd-de7f-6827009a1d9f"
      },
      "source": [
        "# my_test_seq = my_tokenizer.texts_to_sequences(my_test_clean)\n",
        "# print(\"my_test_seq is complete\")\n",
        "\n",
        "my_test_seq = my_tokenizer.gen_list(my_test_clean, with_dot=True)\n",
        "print(\"my_test_seq is complete\")\n",
        "\n",
        "my_test_pad = pad_sequences(my_test_seq, maxlen = 100)\n",
        "print(\"my_test_pad is complete.\")\n",
        "\n",
        "my_x_test = my_test_pad"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_test_seq is complete\n",
            "my_test_pad is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbKAENmjraGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b26e0810-725f-4719-c387-cfc19d03dff3"
      },
      "source": [
        "\n",
        "# print(\"my_test_seq is complete\")\n",
        "\n",
        "my_test_seq = my_tokenizer.gen_list(my_test_clean, with_dot=True)\n",
        "print(\"my_test_seq is complete\")\n",
        "\n",
        "my_test_pad = pad_sequences(my_test_seq, maxlen = 100)\n",
        "print(\"my_test_pad is complete.\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_test_seq is complete\n",
            "my_test_pad is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeKb-tLdRjPD",
        "colab_type": "code",
        "outputId": "1b2d8b89-90e5-4f1f-b104-efbba04ebb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# # Find the number of unique tokens\n",
        "# word_index = tokenizer.word_index\n",
        "# print(\"Words in index: %d\" % len(word_index))\n",
        "train_seq = my_tokenizer.gen_list(train_clean, with_dot=True)\n",
        "print(\"my_test_seq is complete\")\n",
        "\n",
        "train_pad = pad_sequences(train_seq, maxlen = 100)\n",
        "print(\"train_pad is complete.\")\n",
        "\n",
        "test_seq = my_tokenizer.gen_list(test_clean, with_dot=True)\n",
        "print(\"test_seq is complete\")\n",
        "\n",
        "test_pad = pad_sequences(test_seq, maxlen = 100)\n",
        "print(\"test_pad is complete.\")\n",
        "\n",
        "\n",
        "# print('train_seq = ',train_seq)\n",
        "# print('test_seq = ',test_seq)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_test_seq is complete\n",
            "train_pad is complete.\n",
            "test_seq is complete\n",
            "test_pad is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYPei8S-qvxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "9fe3da10-6da1-4a8d-b3d1-1e9854eaa083"
      },
      "source": [
        "print('train_clean = ',train_clean[0])\n",
        "print('train_seq[0] = ',train_seq[0])\n",
        "print('train_pad[0] = ',train_pad[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_clean =  stuff going moment mj i ve started listening music watching odd documentary there watched wiz watched moonwalker again. maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent. moonwalker part biography part feature film remember going see cinema originally released. subtle messages mj s feeling towards press also obvious message drugs bad m kay. visually impressive course michael jackson unless remotely like mj anyway going hate find boring. may call mj egotist consenting making movie mj fans would say made fans true really nice him. the actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord. wants mj dead bad beyond me. mj overheard plans nah joe pesci s character ranted wanted people know supplying drugs etc dunno maybe hates mj s music. lots cool things like mj turning car robot whole speed demon sequence. also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene. bottom line movie people like mj one level another which think people . not stay away. try give wholesome message ironically mj s bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention i ve gave subject....hmmm well know people different behind closed doors know fact. either extremely nice stupid guy one sickest liars. hope latter.\n",
            "train_seq[0] =  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 19, 29, 30, 31, 32, 33, 18, 16, 34, 35, 34, 36, 37, 38, 2, 39, 40, 41, 42, 18, 43, 44, 4, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 18, 56, 57, 58, 59, 60, 61, 62, 63, 4, 64, 2, 65, 66, 67, 18, 68, 69, 4, 70, 71, 72, 73, 4, 74, 75, 76, 77, 74, 78, 26, 79, 80, 18, 81, 82, 36, 37, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 18, 98, 4, 99, 53, 100, 101, 18, 4, 102, 103, 104, 91, 92, 45, 105, 106, 107, 108, 109, 110, 52, 111, 112, 19, 113, 4, 45, 9, 18, 114, 27, 115, 63, 4, 116, 117, 118, 119, 120, 121, 90, 18, 49, 122, 123, 124, 125, 126, 127, 128, 53, 90, 129, 130, 65, 131, 132, 133, 134, 135, 119, 136, 137, 138, 139, 140, 18, 141, 142, 73, 108, 63, 4, 132, 143, 144, 145, 146, 108, 18, 147, 148, 149, 18, 150, 151, 152, 51, 153, 4, 45, 154, 155, 73, 156, 59, 60, 157, 132, 158, 108, 159, 160, 161, 32, 162, 163, 5, 6, 164, 165, 18, 18, 18, 18, 166, 162, 109, 108, 167, 168, 169, 170, 109, 171, 18, 172, 173, 79, 174, 24, 132, 175, 176, 18, 177, 178, 18]\n",
            "train_pad[0] =  [120 121  90  18  49 122 123 124 125 126 127 128  53  90 129 130  65 131\n",
            " 132 133 134 135 119 136 137 138 139 140  18 141 142  73 108  63   4 132\n",
            " 143 144 145 146 108  18 147 148 149  18 150 151 152  51 153   4  45 154\n",
            " 155  73 156  59  60 157 132 158 108 159 160 161  32 162 163   5   6 164\n",
            " 165  18  18  18  18 166 162 109 108 167 168 169 170 109 171  18 172 173\n",
            "  79 174  24 132 175 176  18 177 178  18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsE0s83bRjPo",
        "colab_type": "code",
        "outputId": "17d13a18-1b51-4263-83b8-9a04ee4f7d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Pad and truncate the questions so that they all have the same length.\n",
        "max_review_length = 200\n",
        "\n",
        "train_pad = pad_sequences(train_seq, maxlen = max_review_length)\n",
        "print(\"train_pad is complete.\")\n",
        "\n",
        "test_pad = pad_sequences(test_seq, maxlen = max_review_length)\n",
        "print(\"test_pad is complete.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_pad is complete.\n",
            "test_pad is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37mLpWp0crLA",
        "colab_type": "code",
        "outputId": "923fcd93-9f54-4512-e33c-b4402d809237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "my_test_pad = pad_sequences(my_test_seq, maxlen = max_review_length)\n",
        "print(\"my_test_pad is complete.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_test_pad is complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKDSYjuWUQWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7960da85-6966-4731-bb15-f75e48e489d5"
      },
      "source": [
        "print('train_pad = ',train_pad.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_pad =  (25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za8QZrMDRjP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the training and validation sets\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train_pad, train_data.sentiment, test_size = 0.15, random_state = 2)\n",
        "x_test = test_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW4HuWAN_G4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d8dfeb36-df09-4afb-e7d2-e01352105e17"
      },
      "source": [
        "train_pad[0]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 48,  49,  50,  51,  52,  53,  54,  55,  18,  56,  57,  58,  59,\n",
              "        60,  61,  62,  63,   4,  64,   2,  65,  66,  67,  18,  68,  69,\n",
              "         4,  70,  71,  72,  73,   4,  74,  75,  76,  77,  74,  78,  26,\n",
              "        79,  80,  18,  81,  82,  36,  37,  83,  84,  85,  86,  87,  88,\n",
              "        89,  90,  91,  92,  93,  94,  95,  96,  97,  18,  98,   4,  99,\n",
              "        53, 100, 101,  18,   4, 102, 103, 104,  91,  92,  45, 105, 106,\n",
              "       107, 108, 109, 110,  52, 111, 112,  19, 113,   4,  45,   9,  18,\n",
              "       114,  27, 115,  63,   4, 116, 117, 118, 119, 120, 121,  90,  18,\n",
              "        49, 122, 123, 124, 125, 126, 127, 128,  53,  90, 129, 130,  65,\n",
              "       131, 132, 133, 134, 135, 119, 136, 137, 138, 139, 140,  18, 141,\n",
              "       142,  73, 108,  63,   4, 132, 143, 144, 145, 146, 108,  18, 147,\n",
              "       148, 149,  18, 150, 151, 152,  51, 153,   4,  45, 154, 155,  73,\n",
              "       156,  59,  60, 157, 132, 158, 108, 159, 160, 161,  32, 162, 163,\n",
              "         5,   6, 164, 165,  18,  18,  18,  18, 166, 162, 109, 108, 167,\n",
              "       168, 169, 170, 109, 171,  18, 172, 173,  79, 174,  24, 132, 175,\n",
              "       176,  18, 177, 178,  18], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS5-su-IdCCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_x_test = my_test_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdLpkkE5RjQF",
        "colab_type": "code",
        "outputId": "5c5d803d-1f3c-4ec2-e53d-caf02a0822c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Inspect the shape of the data\n",
        "print(x_train.shape)\n",
        "print(x_valid.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21250, 200)\n",
            "(3750, 200)\n",
            "(25000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIuqOg2fdE2i",
        "colab_type": "code",
        "outputId": "6ac9a24b-c531-4fef-e446-f53a59f0b5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('my_x_test = ',my_x_test,my_x_test.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_x_test =  [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0   26\n",
            "   741   73   18 3595]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0 1705   73  393   18  927   18\n",
            "   221 1484  360  158]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0   73 6201   18   53   18\n",
            "   809  689   18 2783]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0  228  688\n",
            "    17   18  360   53]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0  146   73 1565]] (5, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erk6EI3Yfdc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_ones_sparse(input_arr):\n",
        "  total_res = []\n",
        "  for i in range(input_arr.shape[0]):\n",
        "    word_list = []  \n",
        "    for j in range(input_arr.shape[1]):\n",
        "      if input_arr[i][j] == dot_index:\n",
        "        word_list.append(np.ones(10))\n",
        "      else:\n",
        "        word_list.append(np.zeros(10))\n",
        "    total_res.append(word_list)\n",
        "  return np.array(total_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0MBjwynZQtP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "bf3aa4b3-9d92-481f-930b-33dc172cae71"
      },
      "source": [
        "my_test_sparse = gen_ones_sparse(my_test_pad)\n",
        "print('my_test_sparse = ',my_test_sparse)\n",
        "print('my_test_sparse shape = ',my_test_sparse.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my_test_sparse =  [[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "my_test_sparse shape =  (5, 200, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv4OV3SwfguA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "111a1ec6-666f-42ed-e110-24225e6e3356"
      },
      "source": [
        "x_train_sparse = gen_ones_sparse(x_train)\n",
        "print('x_train_sparse shape = ',x_train_sparse.shape)\n",
        "\n",
        "x_test_sparse = gen_ones_sparse(x_test)\n",
        "print('x_test_sparse shape = ',x_test_sparse.shape)\n",
        "\n",
        "x_valid_sparse = gen_ones_sparse(x_valid)\n",
        "print('x_valid_sparse shape = ',x_valid_sparse.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_sparse shape =  (21250, 200, 10)\n",
            "x_test_sparse shape =  (25000, 200, 10)\n",
            "x_valid_sparse shape =  (3750, 200, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUO8eLYVoJq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "0fa71ea9-c61d-4e1a-a17d-30cc7b16280d"
      },
      "source": [
        "print('x_train = ',x_train[1])\u001d\n",
        "print('x_train_sparse = ',x_train_sparse\u001d.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train =  [ 5390 32474   132  4177  6924   810  2494   722 22296    75   759 11609\n",
            "    18 25523  8641    18   412 15792   179  8227    40   686   394  1254\n",
            "    18    73  5261   759   513   340  2356  1986   802  1313    18  4509\n",
            "   379   468   132  4291   215   206  1238  9022  5058  1944    18    26\n",
            "    26   544  1705    73   306  1438  2110  2295   306    39  2110  2475\n",
            "    18    17   122 55144    73  7462 53438 30951 43962    18  3172  3283\n",
            " 14955  1370    73    45  8121 64904 20605   543 23544 20605  9898   214\n",
            "  1155   543 23544    73    45  2381 20605 12073  8121    18    79    18\n",
            "  1916    53   543    73  4061   219    18   245 20605  1149  2484  2905\n",
            " 24292    63 39566 39567    29    73   215 14588  1433 24492   769 25806\n",
            "    18  4087  2095  1611  5422 12133   405   758  7932   101    18 18105\n",
            "  1284   506  1611  8523    18  9923   842   625  1272 20605  7094  3367\n",
            "  5808   168 18019  7196    18 10099  4345 18697   363 15664   722  2044\n",
            "   677 20605  9335   759   393 15885 48741  2116  5872  8851  2227  3150\n",
            "   132 29663  7200  1105   338    18   219   222   533   688    73  7786\n",
            "    18    21  1218  3553  1295  4609 38368  3729  3982 20605 24095 31300\n",
            "  1188 14830 24492 44663  1234    18   312    18]\n",
            "x_train_sparse =  (21250, 200, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NbDAVzD8fF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KscdqUuBRjQJ",
        "colab_type": "text"
      },
      "source": [
        "# Build and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z54QcY7UFwA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The default parameters of the model\n",
        "# n_words = len(word_index)\n",
        "embed_size = 300\n",
        "batch_size = 250\n",
        "lstm_size = 128\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "multiple_fc = False\n",
        "fc_units = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7iBvTN_RjQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(x, y, batch_size):\n",
        "    '''Create the batches for the training and validation data'''\n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiELKMhjRjQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_batches(x, batch_size):\n",
        "    '''Create the batches for the testing data'''\n",
        "    n_batches = len(x)//batch_size\n",
        "    x = x[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4OGFRL5RjQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cell(lstm_size, keep_prob):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "    return drop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mYPhN9b-RjQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, \n",
        "              dropout, learning_rate, multiple_fc, fc_units, n_prototypes = 10):\n",
        "    '''Build the Recurrent Neural Network'''\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Declare placeholders we'll feed into the graph\n",
        "    with tf.name_scope('inputs'):\n",
        "        inputs = tf.placeholder(tf.int32, [batch_size, max_review_length], name='inputs')\n",
        "\n",
        "    with tf.name_scope('sent_ind'):\n",
        "        sent_ind = tf.placeholder(tf.\u001dfloat32, [batch_size, max_review_length, n_prototypes], name='sent_ind')\n",
        "\n",
        "    with tf.name_scope('labels'):\n",
        "        labels = tf.placeholder(tf.int32, [batch_size, 1], name='labels')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "    # Create the embeddings\n",
        "    with tf.name_scope(\"embeddings\"):\n",
        "        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
        "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
        "\n",
        "    # Build the RNN layers\n",
        "    with tf.name_scope(\"RNN_layers\"):\n",
        "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
        "    \n",
        "    # Set the initial state\n",
        "    with tf.name_scope(\"RNN_init_state\"):\n",
        "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "    # Run the data through the RNN layers\n",
        "    with tf.name_scope(\"RNN_forward\"):\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
        "                                                 initial_state=initial_state)\n",
        "\n",
        "    with tf.name_scope(\"prototype\"):\n",
        "        pos_prototypes = tf.Variable(tf.random_normal([1, lstm_size], stddev=0.03))\n",
        "        for i in range(n_prototypes - 5):\n",
        "            prototype = tf.Variable(tf.random_normal([1, lstm_size], stddev=0.03))\n",
        "            pos_prototypes = tf.concat([pos_prototypes, prototype], 0)\n",
        "\n",
        "        neg_prototypes = tf.Variable(tf.random_normal([1, lstm_size], stddev=0.03))\n",
        "        for i in range(3):\n",
        "            prototype = tf.Variable(tf.random_normal([1, lstm_size], stddev=0.03))\n",
        "            neg_prototypes = tf.concat([neg_prototypes, prototype], 0)\n",
        "\n",
        "        prototypes = tf.concat([pos_prototypes,neg_prototypes],0)\n",
        "\n",
        "        x1 = tf.expand_dims(outputs, 2)\n",
        "        x1 = tf.broadcast_to(x1, [outputs.shape[0].value, outputs.shape[1].value, 10, lstm_size])\n",
        "\n",
        "        # # x2 = tf.broadcast_to(prototypes, [outputs.shape[1].value, 10, lstm_size])\n",
        "        x2 = tf.broadcast_to(prototypes, [outputs.shape[0].value, outputs.shape[1].value, 10, lstm_size])\n",
        "\n",
        "        x3 = x1 - x2\n",
        "        x4 = x3 * x3\n",
        "        distances = tf.reduce_sum(x4 , axis=3)\n",
        "        \n",
        " \n",
        "        # create sparse distances\n",
        "        sparse_dist = distances * sent_ind\n",
        "        max1 = tf.reduce_max(sparse_dist, axis=1)\n",
        "        max2 = tf.expand_dims(max1, 1) + 1\n",
        "        # print('[db] max2 = ',max2)\n",
        "        # print('[db] sparse_dist = ',sparse_dist)\n",
        "        max3 = max2 * tf.ones_like(sparse_dist)\n",
        "        \n",
        "        max4 = tf.where(sparse_dist > 0, sparse_dist, max2 * tf.ones_like(sparse_dist))\n",
        "        max_index = tf.argmin(tf.where(distances > 0, distances, max2 * tf.ones_like(distances)), axis=1)\n",
        "        max_reduce_distances = tf.reduce_min(max4, axis=1)  \n",
        "        print('[db] max_reduce_distances = ',max_reduce_distances)  \n",
        "    \n",
        "    # Create the fully connected layers\n",
        "    with tf.name_scope(\"fully_connected\"):\n",
        "        \n",
        "        # Initialize the weights and biases\n",
        "        # weights = tf.truncated_normal_initializer(stddev=0.1)\n",
        "        # weights = tf.get_variable(\"weights\", initializer=tf.constant([1]))\n",
        "        weights = tf.constant_initializer([1])\n",
        "        biases = tf.zeros_initializer()\n",
        "        \n",
        "        # print('[db] outputs = ',outputs)\n",
        "        # print('[db] outputs[:,-1] = ',outputs[:,-1])\n",
        "\n",
        "        dense = tf.contrib.layers.fully_connected(max_reduce_distances,\n",
        "                                                  num_outputs = fc_units,\n",
        "                                                  activation_fn = tf.sigmoid,\n",
        "                                                  weights_initializer = weights,\n",
        "                                                  biases_initializer = biases)\n",
        "\n",
        "        # dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
        "        \n",
        "        # Depending on the iteration, use a second fully connected layer\n",
        "        if multiple_fc == True:\n",
        "            dense = tf.contrib.layers.fully_connected(dense,\n",
        "                                                      num_outputs = fc_units,\n",
        "                                                      activation_fn = tf.sigmoid,\n",
        "                                                      weights_initializer = weights,\n",
        "                                                      biases_initializer = biases)\n",
        "            dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
        "    \n",
        "    # Make the predictions\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.contrib.layers.fully_connected(dense, \n",
        "                                                        num_outputs = 1, \n",
        "                                                        activation_fn=tf.sigmoid,\n",
        "                                                        weights_initializer = weights,\n",
        "                                                        biases_initializer = biases)\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "    \n",
        "    # Calculate the cost\n",
        "    with tf.name_scope('cost'):\n",
        "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "    \n",
        "    # Train the model\n",
        "    with tf.name_scope('train'):    \n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "    # Determine the accuracy\n",
        "    with tf.name_scope(\"accuracy\"):\n",
        "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        tf.summary.scalar('accuracy', accuracy)\n",
        "    \n",
        "    # Merge all of the summaries\n",
        "    merged = tf.summary.merge_all()    \n",
        "\n",
        "    # Export the nodes \n",
        "    export_nodes = ['inputs', 'sent_ind', 'labels', 'keep_prob', 'initial_state', 'final_state','accuracy',\n",
        "                    'predictions', 'cost', 'optimizer', 'merged']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "    \n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQm1486eRjQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, log_string):\n",
        "    '''Train the RNN'''\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        valid_loss_summary = []\n",
        "        \n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "\n",
        "        print()\n",
        "        print(\"Training Model: {}\".format(log_string))\n",
        "\n",
        "        train_writer = tf.summary.FileWriter('./logs/3/train/{}'.format(log_string), sess.graph)\n",
        "        valid_writer = tf.summary.FileWriter('./logs/3/valid/{}'.format(log_string))\n",
        "\n",
        "        for e in range(epochs):\n",
        "            state = sess.run(model.initial_state)\n",
        "            \n",
        "            # Record progress with each epoch\n",
        "            train_loss = []\n",
        "            train_acc = []\n",
        "            val_acc = []\n",
        "            val_loss = []\n",
        "\n",
        "            with tqdm(total=len(x_train)) as pbar:\n",
        "                for _, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
        "                    # print('[db] x shape ',x,x.shape)\n",
        "                    x_sparse = gen_ones_sparse(x)\n",
        "                    # print('[db] x_sparse shape ',x_sparse,x_sparse.shape)\n",
        "                    feed = {model.inputs: x,\n",
        "                            model.sent_ind: x_sparse,\n",
        "                            model.labels: y[:, None],\n",
        "                            model.keep_prob: dropout,\n",
        "                            model.initial_state: state}\n",
        "                    summary, loss, acc, state, _ = sess.run([model.merged, \n",
        "                                                             model.cost, \n",
        "                                                             model.accuracy, \n",
        "                                                             model.final_state, \n",
        "                                                             model.optimizer], \n",
        "                                                            feed_dict=feed)                \n",
        "                    \n",
        "                    # Record the loss and accuracy of each training batch\n",
        "                    train_loss.append(loss)\n",
        "                    train_acc.append(acc)\n",
        "                    \n",
        "                    # Record the progress of training\n",
        "                    train_writer.add_summary(summary, iteration)\n",
        "                    \n",
        "                    iteration += 1\n",
        "                    pbar.update(batch_size)\n",
        "            \n",
        "            # Average the training loss and accuracy of each epoch\n",
        "            avg_train_loss = np.mean(train_loss)\n",
        "            avg_train_acc = np.mean(train_acc) \n",
        "\n",
        "            val_state = sess.run(model.initial_state)\n",
        "            with tqdm(total=len(x_valid)) as pbar:\n",
        "                for x, y in get_batches(x_valid, y_valid, batch_size):\n",
        "                    # print('[db] x shape ',x,x.shape)\n",
        "                    x_sparse = gen_ones_sparse(x)\n",
        "                    # print('[db] x_sparse shape ',x_sparse,x_sparse.shape)\n",
        "                    feed = {model.inputs: x,\n",
        "                            model.sent_ind: x_sparse,\n",
        "                            model.labels: y[:, None],\n",
        "                            model.keep_prob: 1,\n",
        "                            model.initial_state: val_state}\n",
        "                    summary, batch_loss, batch_acc, val_state = sess.run([model.merged, \n",
        "                                                                          model.cost, \n",
        "                                                                          model.accuracy, \n",
        "                                                                          model.final_state], \n",
        "                                                                         feed_dict=feed)\n",
        "                    \n",
        "                    # Record the validation loss and accuracy of each epoch\n",
        "                    val_loss.append(batch_loss)\n",
        "                    val_acc.append(batch_acc)\n",
        "                    pbar.update(batch_size)\n",
        "            \n",
        "            # Average the validation loss and accuracy of each epoch\n",
        "            avg_valid_loss = np.mean(val_loss)    \n",
        "            avg_valid_acc = np.mean(val_acc)\n",
        "            valid_loss_summary.append(avg_valid_loss)\n",
        "            \n",
        "            # Record the validation data's progress\n",
        "            valid_writer.add_summary(summary, iteration)\n",
        "\n",
        "            # Print the progress of each epoch\n",
        "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
        "                  \"Train Loss: {:.3f}\".format(avg_train_loss),\n",
        "                  \"Train Acc: {:.3f}\".format(avg_train_acc),\n",
        "                  \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n",
        "                  \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n",
        "\n",
        "            # Stop training if the validation loss does not decrease after 3 epochs\n",
        "            if avg_valid_loss > min(valid_loss_summary):\n",
        "                print(\"No Improvement.\")\n",
        "                stop_early += 1\n",
        "                if stop_early == 3:\n",
        "                    break   \n",
        "            \n",
        "            # Reset stop_early if the validation loss finds a new low\n",
        "            # Save a checkpoint of the model\n",
        "            else:\n",
        "                print(\"New Record!\")\n",
        "                stop_early = 0\n",
        "                checkpoint = \"./sentiment_{}.ckpt\".format(log_string)\n",
        "                saver.save(sess, checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxv9gmC3RjQ9",
        "colab_type": "code",
        "outputId": "62482d01-5e24-4c17-d34b-983f4a78ce20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        }
      },
      "source": [
        "# Train the model with the desired tuning parameters\n",
        "for lstm_size in [64]:\n",
        "    for multiple_fc in [False]:\n",
        "        for fc_units in [128]:\n",
        "            log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,\n",
        "                                                      multiple_fc,\n",
        "                                                      fc_units)\n",
        "            model = build_rnn(n_words = n_words, \n",
        "                              embed_size = embed_size,\n",
        "                              batch_size = batch_size,\n",
        "                              lstm_size = lstm_size,\n",
        "                              num_layers = num_layers,\n",
        "                              dropout = dropout,\n",
        "                              learning_rate = learning_rate,\n",
        "                              multiple_fc = multiple_fc,\n",
        "                              fc_units = fc_units)            \n",
        "            train(model, epochs, log_string)\n",
        "            pass"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[db] max_reduce_distances =  Tensor(\"prototype/Min:0\", shape=(250, 10), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21250 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: ru=64,fcl=False,fcu=128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|▍         | 1000/21250 [00:03<01:18, 256.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-19605254e914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                               \u001b[0mmultiple_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiple_fc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                               fc_units = fc_units)            \n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-d21efeb37d00>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, log_string)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;31m# print('[db] x shape ',x,x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mx_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_ones_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0;31m# print('[db] x_sparse shape ',x_sparse,x_sparse.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     feed = {model.inputs: x,\n",
            "\u001b[0;32m<ipython-input-34-fbaf92b693b1>\u001b[0m in \u001b[0;36mgen_ones_sparse\u001b[0;34m(input_arr)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0minput_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdot_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mword_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-oYPXhJRjRF",
        "colab_type": "text"
      },
      "source": [
        "# Make the Predictions (Need modifications)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-JIaeGpGVva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('x_test = ',x_test,x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz22HKI8cyFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUn36dgKcye9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwVbIMfZegKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('my_x_test = ',my_x_test,my_x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFu1Gt0RjRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(lstm_size, multiple_fc, fc_units, checkpoint):\n",
        "    '''Predict the sentiment of the testing data'''\n",
        "    \n",
        "    # Record all of the predictions\n",
        "    all_preds = []\n",
        "\n",
        "    model = build_rnn(n_words = n_words, \n",
        "                      embed_size = embed_size,\n",
        "                      batch_size = batch_size,\n",
        "                      lstm_size = lstm_size,\n",
        "                      num_layers = num_layers,\n",
        "                      dropout = dropout,\n",
        "                      learning_rate = learning_rate,\n",
        "                      multiple_fc = multiple_fc,\n",
        "                      fc_units = fc_units) \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        saver = tf.train.Saver()\n",
        "        # Load the model\n",
        "        saver.restore(sess, checkpoint)\n",
        "        test_state = sess.run(model.initial_state)\n",
        "        for _, x in enumerate(get_test_batches(x_test, batch_size), 1):\n",
        "            print(\"[mydb] x make predictions shape = \",x.shape)\n",
        "            feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1,\n",
        "                    model.initial_state: test_state}\n",
        "            predictions = sess.run(model.predictions, feed_dict=feed)\n",
        "            for pred in predictions:\n",
        "                all_preds.append(float(pred))\n",
        "                \n",
        "    return all_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeL7S20Kemv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions_my_test(lstm_size, multiple_fc, fc_units, checkpoint):\n",
        "    '''Predict the sentiment of the testing data'''\n",
        "    \n",
        "    # Record all of the predictions\n",
        "    all_preds = []\n",
        "\n",
        "    model = build_rnn(n_words = n_words, \n",
        "                      embed_size = embed_size,\n",
        "                      batch_size = batch_size,\n",
        "                      lstm_size = lstm_size,\n",
        "                      num_layers = num_layers,\n",
        "                      dropout = dropout,\n",
        "                      learning_rate = learning_rate,\n",
        "                      multiple_fc = multiple_fc,\n",
        "                      fc_units = fc_units) \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        saver = tf.train.Saver()\n",
        "        # Load the model\n",
        "        saver.restore(sess, checkpoint)\n",
        "        test_state = sess.run(model.initial_state)\n",
        "#         print('[mydb]my_x_test = ',my_x_test)\n",
        "#         print('get_test_batches(my_x_test, batch_size) = ',get_test_batches(my_x_test, batch_size))\n",
        "#         for x in my_x_test:\n",
        "        x = np.zeros(shape=(250,200))\n",
        "        for i,e in enumerate(my_x_test):\n",
        "           x[i] = e\n",
        "#         x = np.array(my_x_test)\n",
        "#         print('[db] x = ',x,x.shape)\n",
        "        feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1,\n",
        "                    model.initial_state: test_state}\n",
        "        predictions, _final_states = sess.run([model.predictions,model.final_state], feed_dict=feed)\n",
        "        for pred in predictions:\n",
        "                all_preds.append(float(pred))\n",
        "                \n",
        "    return all_preds, _final_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FglmmrICes-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIJl01bdRjRQ",
        "colab_type": "text"
      },
      "source": [
        "I am going to compare the results of the best three models, based on the validation data. Then average the predictions of these three models, which should produce an even better set of predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXW9UhoDRjRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint1 = \"./drive/My Drive/deep_learning/research/sentiment_ru=128,fcl=False,fcu=256.ckpt\"\n",
        "checkpoint2 = \"./drive/My Drive/deep_learning/research/sentiment_ru=128,fcl=False,fcu=128.ckpt\"\n",
        "checkpoint3 = \"./drive/My Drive/deep_learning/research/sentiment_ru=64,fcl=True,fcu=256.ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "194gX67HRjRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions using the best 3 models\n",
        "predictions1 = make_predictions(128, False, 256, checkpoint1)\n",
        "predictions2 = make_predictions(128, False, 128, checkpoint2)\n",
        "predictions3 = make_predictions(64, True, 256, checkpoint3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmrMcV6so03a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('predictions1 = ',predictions1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z5-tW9yeyuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions using the best 3 models\n",
        "# my_predictions1 = make_predictions_my_test(128, False, 256, checkpoint1)\n",
        "# my_predictions2 = make_predictions_my_test(128, False, 128, checkpoint2)\n",
        "my_predictions3 = make_predictions_my_test(64, True, 256, checkpoint3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJYk-ApCGM8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6LO4yyLGBHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_test = ['I really love this movie. It is awesome',\n",
        "           'I enjoy this movie so much. What a masterpiece',\n",
        "           'Everything is excellent, the actors are very talented',\n",
        "           'This movie sucks. It is too bad',\n",
        "           'What a waste of time. It is terrible',\n",
        "           'I will never watch it again. Why the actors are so bad',\n",
        "           'I think this movie is fine']\n",
        "\n",
        "my_test = ['it is excellent',\n",
        "           'what a masterpiece',\n",
        "           'everything is great',\n",
        "           'it sucks',\n",
        "           'What a waste of time',\n",
        "           'it is terrible',\n",
        "           'it is ok']\n",
        "\n",
        "my_test_clean = []\n",
        "for review in my_test:\n",
        "    my_test_clean.append(clean_text(review))\n",
        "\n",
        "my_test_seq = tokenizer.texts_to_sequences(my_test_clean)\n",
        "print(\"my_test_seq is complete\")\n",
        "\n",
        "my_test_pad = pad_sequences(my_test_seq, maxlen = max_review_length)\n",
        "print(\"my_test_pad is complete.\")\n",
        "\n",
        "my_x_test = my_test_pad\n",
        "\n",
        "(my_predictions3, final_states) = make_predictions_my_test(64, True, 256, checkpoint3)\n",
        "print('my_predictions3 = ',my_predictions3[:len(my_test)])\n",
        "dd_preds = []\n",
        "for p in my_predictions3[:len(my_test)]:\n",
        "  dd_preds.append([p, 1 - p])\n",
        "\n",
        "(c,h) = final_states[1]\n",
        "# print('final_states = ',final_states[1])\n",
        "print('c = ',c.shape)\n",
        "print('h = ',h.shape)\n",
        "# print('dd_preds = ',dd_preds)\n",
        "dd_preds = np.array(dd_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91VJSYjKJN2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = []\n",
        "for i in range(len(my_test)):\n",
        "  res.append(h[i])\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "X_2d = tsne.fit_transform(res)\n",
        "\n",
        "print('X_2d = ',X_2d)\n",
        "print('X_2d = ',X_2d[:,0])\n",
        "print('X_2d = ',X_2d[:,1])\n",
        "# print('res = ',res)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X_2d[:,0], X_2d[:,1])\n",
        "\n",
        "\n",
        "for i, txt in enumerate(my_test):\n",
        "    plt.annotate(txt, (X_2d[:,0][i], X_2d[:,1][i]))\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mb1Y0r0znqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(dd_preds[:,0], dd_preds[:,1])\n",
        "\n",
        "\n",
        "for i, txt in enumerate(my_test):\n",
        "    plt.annotate(txt, (dd_preds[:,0][i], dd_preds[:,1][i]))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XOydTTWev5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "==========================\n",
        "tSNE to visualize digits\n",
        "==========================\n",
        "\n",
        "Here we use :class:`sklearn.manifold.TSNE` to visualize the digits\n",
        "datasets. Indeed, the digits are vectors in a 8*8 = 64 dimensional space.\n",
        "We want to project them in 2D for visualization. tSNE is often a good\n",
        "solution, as it groups and separates data points based on their local\n",
        "relationship.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "############################################################\n",
        "# Load the iris data\n",
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "# Take the first 500 data points: it's hard to see 1500 points\n",
        "X = digits.data[:500]\n",
        "y = digits.target[:500]\n",
        "\n",
        "############################################################\n",
        "# Fit and transform with a TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "############################################################\n",
        "# Project the data in 2D\n",
        "X_2d = tsne.fit_transform(X)\n",
        "\n",
        "############################################################\n",
        "# Visualize the data\n",
        "target_ids = range(len(digits.target_names))\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(6, 5))\n",
        "colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple'\n",
        "for i, c, label in zip(target_ids, colors, digits.target_names):\n",
        "    plt.scatter(X_2d[y == i, 0], X_2d[y == i, 1], c=c, label=label)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw_SbknTRjRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Average the best three predictions\n",
        "predictions_combined = (pd.DataFrame(predictions1) + pd.DataFrame(predictions2) + pd.DataFrame(predictions3))/3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woph-DdwRjRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_submission(predictions, string):\n",
        "    '''write the predictions to a csv file'''\n",
        "    submission = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":predictions})\n",
        "    submission.to_csv(\"submission_{}.csv\".format(string), index=False, quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyI0vrdoRjRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_submission(predictions1, \"ru=128,fcl=False,fcu=256\") \n",
        "write_submission(predictions2, \"ru=128,fcl=False,fcu=128\") \n",
        "write_submission(predictions3, \"ru=64,fcl=True,fcu=256\") \n",
        "write_submission(predictions_combined.ix[:,0], \"combined\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "KnHsUlW4RjRp",
        "colab_type": "text"
      },
      "source": [
        "The results of the predictions are as follows (Kaggle used area under the ROC curve to evaluation submissions):\n",
        "- Predictions1: 0.919\n",
        "- Predictions2: 0.914\n",
        "- Predictions3: 0.916\n",
        "- Combined Predictions: 0.935"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "n7lhjshDRjRq",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4_vFTV2RjRr",
        "colab_type": "text"
      },
      "source": [
        "I am rather pleased by how this analysis has finished. Now I am much more confident in using TensorBoard to improve the design of a model and I have achieved rather good results. The combined predictions' submission ranks 206 out of 578, top 35.6%. This result could have been improved by using a larger model, using pretrained vectors (such as GloVe), and using an ensemble of more predictions. Although it would be nice to carry out these efforts and improve my results, I feel that would not be the best use of my time. There are more complicated projects that I would like to work on now, rather than training this model multiple times for a competition that has already concluded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4wPEOEvDRjRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}